import os
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
import sqlite3


DOCS_FOLDER = "docs"

FILTER_PHRASES = [
    "–í–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã", "–õ–∏—Å—Ç –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è", "–§.–ò.–û.",
    "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–æ", "–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ", "–ø–æ–¥–ø–∏—Å—å", "–æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω", "–æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∞"
]

def is_garbage(text):
    if len(text.strip()) < 30:
        return True
    if any(p.lower() in text.lower() for p in FILTER_PHRASES):
        return True
    if sum(c in ".,:-‚Äì_ \n" for c in text) / len(text) > 0.8:
        return True
    return False


def load_documents(folder):
    documents = []
    for filename in os.listdir(folder):
        filepath = os.path.join(folder, filename)
        try:
            if filename.endswith(".pdf"):
                loader = PyPDFLoader(filepath)
            elif filename.endswith(".docx"):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
                import zipfile
                if not zipfile.is_zipfile(filepath):
                    print(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª {filename} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DOCX —Ñ–∞–π–ª–æ–º!")
                    continue
                loader = Docx2txtLoader(filepath)
            else:
                continue

            #print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}")
            loaded_docs = loader.load()

            for doc in loaded_docs:
                doc.metadata["source"] = filename
                documents.append(doc)

            #print(f"‚úÖ –§–∞–π–ª {filename} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")

        except Exception as e:
            print(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {filename}: {str(e)}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –¥—Ä—É–≥–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏
            continue

    return documents

def split_and_filter_documents(documents):
    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
    PRIORITY_FILES = {"rules.docx"}

    final_chunks = []
    for doc in documents:
        filename = doc.metadata.get("source", "").lower()
        is_priority = filename in PRIORITY_FILES

        if is_priority:
            # –î–õ–Ø –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–• –§–ê–ô–õ–û–í - –ë–û–õ–ï–ï –ö–†–£–ü–ù–´–ï –ß–ê–ù–ö–ò
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=3000,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –≤ 2 —Ä–∞–∑–∞
                chunk_overlap=500,  # –ë–æ–ª—å—à–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                separators=["\n\n**", "\n\n", "\n", ".", " "]  # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            )
        else:
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=300,
                chunk_overlap=100,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )

        chunks = splitter.split_documents([doc])

        if is_priority:
            # –ù–ï –§–ò–õ–¨–¢–†–£–ï–ú –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            final_chunks.extend(chunks)
            print(f"üéØ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ô: {filename} —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        else:
            filtered = [chunk for chunk in chunks if not is_garbage(chunk.page_content)]
            final_chunks.extend(filtered)

    return final_chunks



def main():
    documents = load_documents(DOCS_FOLDER)
    chunks = split_and_filter_documents(documents)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
    print(f"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(chunks)} —á–∞–Ω–∫–æ–≤.")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑—É
    results = [(chunk.metadata.get("source", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"), chunk.page_content.strip()) for chunk in chunks]

    # –ó–∞–ø–∏—à–µ–º –≤ –±–∞–∑—É
    save_chunks_to_db(results)

    return results


def parse_and_return_chunks():
    documents = load_documents(DOCS_FOLDER)
    chunks = split_and_filter_documents(documents)
    results = [(chunk.metadata.get("source", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"), chunk.page_content.strip()) for chunk in chunks]
    return results

def save_chunks_to_db(chunks):
    conn = sqlite3.connect("bot_data.db")
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS chunks (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        document_name TEXT,
        chunk TEXT
    )
    """)
    cur.execute("DELETE FROM chunks")  # –æ—á–∏—â–∞–µ–º —Å—Ç–∞—Ä–æ–µ
    for docname, text in chunks:
        cur.execute("INSERT INTO chunks (document_name, chunk) VALUES (?, ?)", (docname, text))
    conn.commit()
    conn.close()

if __name__ == "__main__":
    main()
